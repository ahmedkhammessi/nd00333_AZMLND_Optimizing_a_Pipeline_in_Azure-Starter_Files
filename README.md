# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
- We had initially a dataset stored already in a publicly accessible blob storage containing bank customers data. Thoughout the experince we aim at predicting whether the customers will be intersted in new product by basing the marketing campaign on the most influencing parameter in the dataset.

- The best performing model was a VotingEnsemble algorithm (accuracy of 0.91739) resulting from an Azure AutoML run, which has performed better than the first one, trained through Azure HyperDrive on a LogisticRegression algorithm (accuracy of 0.9132).

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
- The first steps are generic, So mainly intialize the workspace and create a compute cluster for model training. Then prepare the data by creating a TabularDataset form the provided CSV file.

- The data was then cleaned, and split using the training script 'train.py'.

- The classifying algorithm used is logistic regression. It is used to estimate discrete values based on a set of independent variables.

- Next, SKLearn estimator was constructed. This estimator will provide a simple way of deploying the training job on the compute target.

- The last step is to provide the Sampling Parameter to run the hyperparameter tuning. The ranges for the inverse of the regularization strength and choices for maximum number of iterations to converge are provided.

- We configure the HyperDrive to set the 'Accuracy' as the primary metric.

- Finally sibmit the experiment and find the best model.

**What are the benefits of the parameter sampler you chose?**

- Random sampling supports discrete and continuous hyperparameters. It supports early termination of low-performance runs and thereby reducing computation costs and speedup up the exploration of the parameter space.

**What are the benefits of the early stopping policy you chose?**

- Bandit policy is based on slack factor and evaluation interval. Bandit terminates runs where the primary metric is not within the specified slack factor compared to the best performing run. Slack factor is the slack allowed with respect to the best performing training run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
- The best model generated by AutoML is a VotingEnsemble method with a pre-processing step that uses MaxAbsScaler as pre-processing step.
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
- The Voting Ensemble model outperform the Logistic Regression one with an accuracy of 0.9181 against 0.9129.

The differences in architecture are huge:

- HyperDrive starts multiple runs each of which trains the LogisticRegression model using different tuples of hyper-parameters
- AutoML starts multiple runs each of which executes complex pipelines with choices of hyper-parameters, models and configuration details
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
- Adapt the imbalanced classes detected in the dataset.
- Increase the max_total_runs parameter of HyperDriveConfig
- Increase the experiment_timeout_minutes parameter of the AutoMLConfig
